{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from arnn import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938278"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('lotr1.txt', 'r').read().lower()\n",
    "text = text.replace('\\xa0', ' ')\n",
    "text = text.replace('/', '')\n",
    "text = text.replace('*', '')\n",
    "#text = text[:600000]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 57\n"
     ]
    }
   ],
   "source": [
    "char_to_idx = {char: idx for (idx, char) in enumerate(list(set(text)))}\n",
    "idx_to_char = {idx: char for (char, idx) in char_to_idx.items()}\n",
    "one_hot_size = len(char_to_idx)\n",
    "print(f'Number of unique characters: {one_hot_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_size:int, vocab_size:int):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, input:torch.Tensor, h_init:torch.Tensor):\n",
    "        _, h = self.rnn(input, h_init)\n",
    "        out = self.fc(h.squeeze(0))\n",
    "\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotDataset:\n",
    "    def __init__(self, seq_len, chr_step, text, char_to_idx):\n",
    "        sequences = []\n",
    "        next_chars = []\n",
    "\n",
    "        for i in range(0, len(text) - seq_len, chr_step):\n",
    "            sequences.append(text[i:i+seq_len]) # input sequence\n",
    "            next_chars.append(text[i+seq_len]) # char to predict\n",
    "        \n",
    "        self.x = torch.zeros(len(sequences), seq_len, one_hot_size) # shape (L, N ,D)\n",
    "        self.y = torch.zeros(len(sequences), one_hot_size)\n",
    "\n",
    "        for i, sentence in enumerate(sequences):\n",
    "            for t, char in enumerate(sentence):\n",
    "                self.x[i, t, char_to_idx[char]] = 1\n",
    "            self.y[i, char_to_idx[next_chars[i]]] = 1\n",
    "\n",
    "        self.x = self.x.to(device)\n",
    "        self.y = self.y.to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx, :, :], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "data = OneHotDataset(60, 3, text, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "hidden_size = 256\n",
    "batch_size = 512\n",
    "lr = 0.001\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.3623299411392837\n",
      "Epoch 1, loss: 1.8960485228149864\n",
      "Epoch 2, loss: 1.6935520607203782\n",
      "Epoch 3, loss: 1.565801881339078\n",
      "Epoch 4, loss: 1.4762569958566645\n",
      "Epoch 5, loss: 1.4096127317306844\n",
      "Epoch 6, loss: 1.3573111243021663\n",
      "Epoch 7, loss: 1.3142419793992113\n",
      "Epoch 8, loss: 1.2772307146200377\n",
      "Epoch 9, loss: 1.2443517622112643\n",
      "Epoch 10, loss: 1.214603691171312\n",
      "Epoch 11, loss: 1.186813774163516\n",
      "Epoch 12, loss: 1.160647168389709\n",
      "Epoch 13, loss: 1.135663877532447\n",
      "Epoch 14, loss: 1.1115712616135367\n",
      "Epoch 15, loss: 1.088163818548237\n",
      "Epoch 16, loss: 1.0655169738685246\n",
      "Epoch 17, loss: 1.0444429727505935\n",
      "Epoch 18, loss: 1.025461225189671\n",
      "Epoch 19, loss: 1.0107575607182742\n",
      "Epoch 20, loss: 1.0022248140918837\n",
      "Epoch 21, loss: 0.9953767347062669\n",
      "Epoch 22, loss: 0.9902115587126799\n",
      "Epoch 23, loss: 0.9823064461667487\n",
      "Epoch 24, loss: 0.9736600941604951\n",
      "Epoch 25, loss: 0.9646115824946014\n",
      "Epoch 26, loss: 0.956083917773882\n",
      "Epoch 27, loss: 0.9513289601635035\n",
      "Epoch 28, loss: 0.9469939417612728\n",
      "Epoch 29, loss: 0.940320583977988\n"
     ]
    }
   ],
   "source": [
    "char_rnn = CharRNN(hidden_size=hidden_size, vocab_size=len(char_to_idx), input_size=one_hot_size).to(device)\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "optim = torch.optim.Adam(char_rnn.parameters(), lr=lr)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    run_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        optim.zero_grad()\n",
    "        out, h = char_rnn(x.transpose(0, 1), None)\n",
    "        l = loss(out, y)\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "\n",
    "        run_loss += l.item()\n",
    "    if verbose:\n",
    "        print(f'Epoch {epoch}, loss: {run_loss/len(loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output shape (batch_size, vocab_size)\n",
    "# input shape (seq_len, batch_size, input_size)\n",
    "\n",
    "def char_to_onehot(char, char_to_idx):\n",
    "    x = torch.zeros(1, 1, one_hot_size).to(device)\n",
    "    x[0, 0, char_to_idx[char]] = 1\n",
    "    \n",
    "    return x\n",
    "\n",
    "def seq_to_onehot(seq, char_to_idx):\n",
    "    x = torch.zeros(len(seq), 1, one_hot_size)\n",
    "    for t, char in enumerate(seq):\n",
    "        x[t, 0, char_to_idx[char]] = 1\n",
    "\n",
    "    return x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, start, char_to_idx, idx_to_char): # generate text from nothing\n",
    "    with torch.no_grad():\n",
    "        for temperature in [0.2, 0.5, 1.2]:\n",
    "            temperature = 0.1\n",
    "            generated = start # start from space\n",
    "            input = seq_to_onehot(generated, char_to_idx)\n",
    "            out, h = model(input, None)\n",
    "            p = F.softmax(out / temperature, dim=1)\n",
    "            idx = torch.distributions.Categorical(p[0]).sample().item()\n",
    "            generated += idx_to_char[idx]\n",
    "            last_char = generated[-1]\n",
    "\n",
    "            for _ in range(length):\n",
    "                out, h = model(char_to_onehot(last_char, char_to_idx), h)\n",
    "                out = torch.divide(out, temperature)\n",
    "                p = F.softmax(out, dim=1)\n",
    "                idx = torch.distributions.Categorical(p[0]).sample().item()\n",
    "                generated += idx_to_char[idx]\n",
    "                last_char = generated[-1]\n",
    "            print(len(generated))\n",
    "            print(f'Temperature: {temperature}\\n{generated}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(char_rnn.state_dict(), 'char_rnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when mr. bilbo baggins of bag end announced that he would sh\n",
      "261\n",
      "Temperature: 0.1\n",
      "when mr. bilbo baggins of bag end announced that he would shotely up the gates of allownsed to the eastern soon. \"we cannot answers and purpuis to see for a while to the eastern or the dwarves how the darkness. \"or it will be there is not the land of aragorn. \"\n",
      "\n",
      "261\n",
      "Temperature: 0.1\n",
      "when mr. bilbo baggins of bag end announced that he would shotely up the stars were stone. it was so unouther to the shire that he was seen and come. and you mean to see what it was in the dark lord of great since to find the days. i am afready sail, as i was t\n",
      "\n",
      "261\n",
      "Temperature: 0.1\n",
      "when mr. bilbo baggins of bag end announced that he would shotely up the stars were read out of the eastern shore. and we will not see no to the hollow between them and stone. it was standing stone. \"then i cannot remome beast a count of mordor. \"but i have see\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = text[:60]\n",
    "print(start)\n",
    "generate_text(char_rnn, 200, start, char_to_idx, idx_to_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
