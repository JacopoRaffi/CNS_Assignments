{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3_2: Echo State Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from esn import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.083964</td>\n",
       "      <td>0.48934</td>\n",
       "      <td>0.35635</td>\n",
       "      <td>0.25024</td>\n",
       "      <td>0.23554</td>\n",
       "      <td>0.029809</td>\n",
       "      <td>0.34099</td>\n",
       "      <td>0.021216</td>\n",
       "      <td>0.035723</td>\n",
       "      <td>0.26082</td>\n",
       "      <td>0.048365</td>\n",
       "      <td>0.40907</td>\n",
       "      <td>0.40877</td>\n",
       "      <td>0.36122</td>\n",
       "      <td>0.074933</td>\n",
       "      <td>0.32980</td>\n",
       "      <td>0.25930</td>\n",
       "      <td>0.48649</td>\n",
       "      <td>0.32450</td>\n",
       "      <td>0.40017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.132850</td>\n",
       "      <td>0.17536</td>\n",
       "      <td>0.37127</td>\n",
       "      <td>0.36481</td>\n",
       "      <td>0.337070</td>\n",
       "      <td>0.20447</td>\n",
       "      <td>0.33003</td>\n",
       "      <td>0.20726</td>\n",
       "      <td>0.18825</td>\n",
       "      <td>0.28343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2        3        4         5        6         7   \\\n",
       "0  0.083964  0.48934  0.35635  0.25024  0.23554  0.029809  0.34099  0.021216   \n",
       "1  0.000000  0.00000  0.00000  0.00000  0.00000  0.000000  0.00000  0.000000   \n",
       "\n",
       "         8        9         10       11       12       13        14       15  \\\n",
       "0  0.035723  0.26082  0.048365  0.40907  0.40877  0.36122  0.074933  0.32980   \n",
       "1  0.000000  0.00000  0.132850  0.17536  0.37127  0.36481  0.337070  0.20447   \n",
       "\n",
       "        16       17       18       19  \n",
       "0  0.25930  0.48649  0.32450  0.40017  \n",
       "1  0.33003  0.20726  0.18825  0.28343  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narma_df = pd.read_csv('../NARMA10.csv', header=None)\n",
    "narma_df.iloc[:, :20] # visualize the first 20 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reservoir(nn.Module):\n",
    "    def __init__(self, input_size:int, hidden_size:int, omhega_in:float, omhega_b:float, rho:float, density:float = 1):\n",
    "        '''\n",
    "        Initialize Echo State Network (ESN) with the given parameters\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_size: int\n",
    "            Size of the input\n",
    "        hidden_size: int\n",
    "            Size of the hidden layer\n",
    "        omhega_in: float\n",
    "            Input scaling\n",
    "        omhega_b: float\n",
    "            Bias scaling\n",
    "        rho: float\n",
    "            Desired Spectral radius of the hidden recurrent layer weight matrix\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        return: -\n",
    "        '''\n",
    "        super(Reservoir, self).__init__()\n",
    "\n",
    "        self.input_scaling = nn.Parameter(torch.tensor(omhega_in), requires_grad=False)\n",
    "        self.rho = nn.Parameter(torch.tensor(rho), requires_grad=False)\n",
    "        self.hidden_size = nn.Parameter(torch.tensor(hidden_size), requires_grad=False)\n",
    "\n",
    "        self.W_in = nn.Parameter(nn.init.uniform_(torch.empty(hidden_size, input_size), -omhega_in, omhega_in), requires_grad=False)\n",
    "        self.bias = nn.Parameter(nn.init.uniform_(torch.empty(hidden_size), -omhega_b, omhega_b), requires_grad=False)\n",
    "\n",
    "        W_h = nn.init.uniform_(torch.empty(hidden_size, hidden_size), -1, 1)\n",
    "        W_h = W_h.div_(torch.linalg.eigvals(W_h).abs().max()).mul_(rho).float() # use in-place operations (div_, mul_) to save memory\n",
    "\n",
    "        self.W_h = nn.Parameter(W_h, requires_grad=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input:torch.Tensor, h_init:torch.Tensor, washout:int = 0) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass through the ESN\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input: torch.Tensor\n",
    "            Input tensor. Input of Shape (L, input size) or (L, N, input size) if input is batched \n",
    "            (L is the length of the sequence, N is the batch size)\n",
    "        h_init: torch.Tensor\n",
    "            Initial hidden state (set to zeros if None)\n",
    "        washout: int\n",
    "            Number of time steps to ignore\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        return: torch.Tensor\n",
    "            Output tensor\n",
    "        '''\n",
    "\n",
    "        timesteps, batch_size, _ = input.shape\n",
    "        h = torch.zeros(batch_size, self.hidden_size) if h_init is None else h_init.copy()\n",
    "        states = []\n",
    "\n",
    "        for t in range(timesteps):\n",
    "            h = F.linear(input[t], self.W_in, self.bias) + F.linear(h, self.W_h)\n",
    "            h = F.tanh(h)\n",
    "            states.append(h)\n",
    "\n",
    "        return torch.stack(states[washout:], dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2926,  0.2173,  0.3035, -0.1396,  0.0698,  0.0783, -0.2219,\n",
       "           0.0921, -0.3867, -0.1676, -0.2341]],\n",
       "\n",
       "        [[-0.0449, -0.0657,  0.4548,  0.1277,  0.1643,  0.0532, -0.2058,\n",
       "           0.0210, -0.4133, -0.2410, -0.4792]],\n",
       "\n",
       "        [[-0.0091, -0.1591,  0.4855,  0.2849,  0.4492, -0.0099, -0.1439,\n",
       "           0.0308, -0.3426, -0.2826, -0.4234]],\n",
       "\n",
       "        [[ 0.0810,  0.0130,  0.4519, -0.0856,  0.2899, -0.2181, -0.4297,\n",
       "          -0.0913, -0.4189, -0.3270, -0.3094]],\n",
       "\n",
       "        [[ 0.0988,  0.0239,  0.4912,  0.2029,  0.2416, -0.0098, -0.3685,\n",
       "          -0.1483, -0.4116, -0.2166, -0.4392]],\n",
       "\n",
       "        [[ 0.0437, -0.0909,  0.4634,  0.1583,  0.3612, -0.0756, -0.3403,\n",
       "          -0.1830, -0.4476, -0.2549, -0.3907]],\n",
       "\n",
       "        [[ 0.0949, -0.0193,  0.4543,  0.0918,  0.3277, -0.1202, -0.3882,\n",
       "          -0.1632, -0.4760, -0.2520, -0.3508]],\n",
       "\n",
       "        [[ 0.0882,  0.0043,  0.4729,  0.0808,  0.2542, -0.1085, -0.4287,\n",
       "          -0.1956, -0.4996, -0.2342, -0.3743]],\n",
       "\n",
       "        [[ 0.0676, -0.0538,  0.4774,  0.2009,  0.3233, -0.0467, -0.3540,\n",
       "          -0.1837, -0.4729, -0.2131, -0.4049]],\n",
       "\n",
       "        [[ 0.0447, -0.1035,  0.4056,  0.2998,  0.4615, -0.0234, -0.1931,\n",
       "          -0.0825, -0.3613, -0.2270, -0.3975]]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = torch.rand(10, 1, 1)\n",
    "r = Reservoir(1, 11, 0.4, 0.5, 0.6)\n",
    "r(cc, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
